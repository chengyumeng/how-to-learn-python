{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66884425-bcd8-49b9-b3c1-336cfbb467b6",
   "metadata": {},
   "source": [
    "# 爬下来的数据的存储和读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270597b-860e-4978-8232-009bdb3af5ec",
   "metadata": {},
   "source": [
    "## 回顾\n",
    "- 爬取帖子列表页\n",
    "- 爬取帖子详情页\n",
    "- [新]存储帖子列表页的信息（标题、URL、作者、时间、阅读量）\n",
    "- [新]对每一条帖子存储内容、所有评论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3d996-9010-4368-a339-8d5bb1d2b7ea",
   "metadata": {},
   "source": [
    "## 新知识\n",
    "- 使用 pandas 操作 CSV\n",
    "- 使用正则表达式进行内容匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "200f4546-2c06-4f4c-8882-8ff7a73b2137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pandas\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/7c/20e737300f9bec011fb79c01d8948bc38c854876aac2da2cfcdd0992b153/pandas-2.1.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2,>=1.23.2 (from pandas)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2e/54/218ce51bb571a70975f223671b2a86aa951e83abfd2a416a3d540f35115c/numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/zhangpenghao/anaconda3/envs/jupyterlab/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhangpenghao/anaconda3/envs/jupyterlab/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/fb/a79efcab32b8a1f1ddca7f35109a50e4a80d42ac1c9187ab46522b2407d7/tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/zhangpenghao/anaconda3/envs/jupyterlab/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.2 pandas-2.1.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c12aa0-c404-42c4-b987-1e3f3e5ca1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 基础包\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 公共 header，可以参见 postman 生成的 Python 代码，这个通常每次使用的时候都是固定的\n",
    "headers = {\n",
    "  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "  'Accept-Language': 'en,zh-CN;q=0.9,zh;q=0.8,zh-TW;q=0.7,fr;q=0.6',\n",
    "  'Cache-Control': 'max-age=0',\n",
    "  'Connection': 'keep-alive',\n",
    "  'Cookie': 'll=\"108288\"; bid=EUIXgN0ZKpc; _pk_id.100001.8cb4=1f708fde13a7265b.1700533653.; __utmc=30149280; viewed=\"36165055\"; douban-fav-remind=1; __yadk_uid=8ArcxBZuzhGsH9PICN0uVjxPJzsUPXf0; _ga=GA1.2.1030669451.1701136885; _gid=GA1.2.1551607325.1701136885; _ga_Y4GN1R87RG=GS1.1.1701136884.1.0.1701136900.0.0.0; __utmz=30149280.1701150588.3.2.utmcsr=localhost:8888|utmccn=(referral)|utmcmd=referral|utmcct=/; ap_v=0,6.0; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1701180456%2C%22http%3A%2F%2Flocalhost%3A8888%2F%22%5D; _pk_ses.100001.8cb4=1; __utma=30149280.1324323786.1700533655.1701175760.1701180456.5; __utmt=1; __utmb=30149280.12.9.1701181143253; bid=qYTbTuF2dfs',\n",
    "  'Referer': 'https://www.douban.com/group/596337/discussion?start=0&type=new',\n",
    "  'Sec-Fetch-Dest': 'document',\n",
    "  'Sec-Fetch-Mode': 'navigate',\n",
    "  'Sec-Fetch-Site': 'same-origin',\n",
    "  'Sec-Fetch-User': '?1',\n",
    "  'Upgrade-Insecure-Requests': '1',\n",
    "  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "  'sec-ch-ua': '\"Google Chrome\";v=\"119\", \"Chromium\";v=\"119\", \"Not?A_Brand\";v=\"24\"',\n",
    "  'sec-ch-ua-mobile': '?0',\n",
    "  'sec-ch-ua-platform': '\"macOS\"'\n",
    "}\n",
    "payload={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c4cf5-38f9-41f7-a633-e5b4df725f17",
   "metadata": {},
   "source": [
    "# 列表页面函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141bf30-74b7-4de5-9e17-6e88e85bf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个新的 DataFrame 并指定列名\n",
    "import pandas as pd\n",
    "def query_page(group, start: int):\n",
    "    url = \"https://www.douban.com/group/{}/discussion?start={}&type=new\".format(group,start)\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # arr = soup.select(\"td.title > a\")\n",
    "    # times = soup.select(\"td.time\")\n",
    "    arr = soup.select(\"table.olt > tr:not(.th)\")\n",
    "    discussions = pd.DataFrame(columns=['url', 'title', 'author', 'author_id', 'reply_count', 'time'])\n",
    "    for item in arr:\n",
    "        tds = item.find_all('td')\n",
    "        href = tds[0].find(\"a\")['href']\n",
    "        title = tds[0].find(\"a\")['title']\n",
    "        author = tds[1].text.replace(\"\\n\", \"\")\n",
    "        author_id = tds[1].find(\"a\")['href'].replace('https://www.douban.com/people/', '').replace('/', '')\n",
    "        reply_count = tds[2].text\n",
    "        tm = tds[3].text\n",
    "        # 向 Dataframe 追加行\n",
    "        row = pd.DataFrame({\n",
    "                'url': href,\n",
    "                'title': title,\n",
    "                'author': author, \n",
    "                'author_id': author_id,\n",
    "                'reply_count': reply_count,\n",
    "                'time': tm,\n",
    "            }, index=[0])\n",
    "        discussions = pd.concat([discussions, row], ignore_index=True)\n",
    "    return discussions\n",
    "pd.concat([query_page(596337, 0),query_page(596337, 30)] , ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d067f-ccb3-4a2f-9920-e28be331bc8b",
   "metadata": {},
   "source": [
    "# 帖子详情函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358d6d2-de20-48bc-baca-3922f53ab881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "details = pd.DataFrame(columns=['url', 'type', 'text', 'imgs', 'author', 'author_id', 'time', 'location'])\n",
    "\n",
    "def query_topic(details: pd.DataFrame, url: str,start=0):\n",
    "    response =requests.request(\"GET\", \"{}?start={}\".format(url, start), headers=headers, data=payload)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    author_data = soup.select(\"span.from > a\")\n",
    "    author_create_time_data = soup.select(\"span.create-time.color-green\")\n",
    "    author_location_data = soup.select(\"span.ip-location\")\n",
    "    author_name, author_id, author_create_time, author_location = '', '', '', ''\n",
    "\n",
    "    if len(author_data) > 0:\n",
    "        author_name = author_data[0].text\n",
    "        author_id = author_data[0]['href'].replace('https://www.douban.com/people/', '').replace('/', '')\n",
    "    if len(author_create_time_data) > 0:\n",
    "        author_create_time = author_create_time_data[0].text\n",
    "    if len(author_location_data) > 0:\n",
    "        author_location = author_location_data[0].text\n",
    "\n",
    "\n",
    "    data = soup.select(\"div.rich-content.topic-richtext > p, div.rich-content.topic-richtext > div\")\n",
    "    # 下面的内容解析了文字也解析了图片，和视频里讲的不一样\n",
    "    if len(data) > 0:\n",
    "        for item in data:\n",
    "            img_tags = item.select(\"img\")\n",
    "            img_urls = [img_tag[\"src\"] for img_tag in img_tags]\n",
    "            row = pd.DataFrame({\n",
    "               'url': url,\n",
    "               'type': '正文', \n",
    "               'text': item.text,\n",
    "               'imgs': \" \".join(img_urls),\n",
    "               'author': author_name,\n",
    "               'author_id': author_id,\n",
    "               'time': author_create_time,\n",
    "               'location': author_location\n",
    "            }, index=[0])\n",
    "            details = pd.concat([details, row], ignore_index=True)\n",
    "    arr = soup.select(\"div.reply-doc.content\")\n",
    "    for item in arr:\n",
    "        img_tags = item.select(\"img\")\n",
    "        img_urls = [img_tag[\"src\"] for img_tag in img_tags]\n",
    "        txt = item.select(\"p.reply-content\")[0].text\n",
    "        author = item.select(\"h4 > a\")[0].text\n",
    "        author_id = item.select(\"h4 > a\")[0]['href'].replace('https://www.douban.com/people/', '').replace('/', '')\n",
    "        pubtime = item.select(\"span.pubtime\")[0].text\n",
    "        pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\s(.*)\"\n",
    "        match = re.match(pattern, pubtime)\n",
    "        if match:\n",
    "            time = match.group(1)\n",
    "            location = match.group(2)\n",
    "        row = pd.DataFrame({\n",
    "               'url': url,\n",
    "               'type': '评论', \n",
    "               'text': txt,\n",
    "               'imgs': \" \".join(img_urls),\n",
    "               'author': author,\n",
    "               'author_id': author_id,\n",
    "               'time': time,\n",
    "               'location': location\n",
    "            }, index=[0])\n",
    "        details = pd.concat([details, row], ignore_index=True)\n",
    "    if len(arr) >= 100:\n",
    "        return query_topic(details, url, start+100)\n",
    "    return details\n",
    "query_topic(details, \"https://www.douban.com/group/topic/288398770/\", 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddcc59-82aa-423e-8f54-da2288316b0f",
   "metadata": {},
   "source": [
    "# Dataframe 与 csv 读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cb2ca7e0-4c53-4d82-8a4d-b69e67ed74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "discussions = pd.concat([query_page(596337, 0),query_page(596337, 30)] , ignore_index=True)\n",
    "discussions.to_csv('discussions.csv', index=False)\n",
    "\n",
    "discussions = pd.read_csv('discussions.csv')\n",
    "for row in discussions.itertuples():\n",
    "    details = pd.DataFrame(columns=['url', 'type', 'text', 'imgs', 'author', 'time', 'location'])\n",
    "    details = query_topic(details, row.url, 0)\n",
    "    pattern = r'/group/topic/(\\d+)/'\n",
    "    topic_id = row.url\n",
    "    match = re.search(pattern, row.url)\n",
    "    if match:\n",
    "        topic_id = match.group(1)\n",
    "    details.to_csv('topic_{}.csv'.format(topic_id), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
